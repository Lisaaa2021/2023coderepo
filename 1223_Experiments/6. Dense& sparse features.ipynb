{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.2\n",
    "\n",
    "## Goal\n",
    "\n",
    "The performance of machine learning systems directly depends on the quality of input features. In this exercise, you will investigate the impact of individual features on a system for named entity recognition: what does the inclusion of each individual feature do to the results? And what happens when they are combined?\n",
    "\n",
    "\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "This exercise made use of examples from the following exercise (in the HLT course):\n",
    "\n",
    "https://github.com/cltl/ma-hlt-labs/\n",
    "\n",
    "Lab3: machine learning\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "This notebook will provide the code for running the experiments outlined above. You will only need to make minor adaptations to run the feature ablation analysis. This notebook was developed for another more introductory course where students did not need to generate their own features. Please take that into account while reading this code (i.e. you can use this as an example, but it will not work one-to-one on your own data).\n",
    "\n",
    "The notebooks and set up have been designed for educational purposes: design choices are based on clearly illustrating what is going on and facilitating the exercises.\n",
    "\n",
    "## The Data\n",
    "\n",
    "The data of the original assignment been preprocessed to make some useful features directly availabe (as you were recommended to do as well in Assignment 2).\n",
    "\n",
    "The format of the conll files provided to the students in this original exercise was:\n",
    "\n",
    "Token  Preceding_token  Capitalization  POS-tag  Chunklabel  Goldlabel\n",
    "\n",
    "The first lines look like this:\n",
    "\n",
    "-DOCSTART-      FULLCAP -X-     -X-     O\n",
    "  \n",
    "EU              FULLCAP NNP     B-NP    B-ORG\n",
    "\n",
    "rejects EU      LOWCASE VBZ     B-VP    O\n",
    "\n",
    "Preceding_token: \n",
    "This column provides the token preceding the current token. (This is an empty space if there is no previous token).\n",
    "\n",
    "Capitalization: \n",
    "This column provides information on the capitalization of the token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "We will make use of the following packages:\n",
    "\n",
    "* scikit-learn : provides lots of useful implementations for machine learning (and has relatively good documentation!)\n",
    "* csv: a light-weight package to deal with data represented in csv (or related formats such as tsv)\n",
    "* gensim: a useful package for working with word embeddings\n",
    "* numpy: a packages that (among others) provides useful datastructures and operations to work with vectors\n",
    "\n",
    "Some notes on design decisions (feel free to ignore these if this is all new to you):\n",
    "\n",
    "* We are using csv rather than (the more common) pandas for working with the conll files, because pandas standardly applies type conversion, which we do not want when dealing with text that contains numbers (fixing this will make the code look more complex).\n",
    "* scikit-learn provides several machine learning algorithms, but this is not the focus of this exercise. We are using logistic regression, because it serves the purpose of our experiments and is relatively efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell imports all the modules we'll need. Make sure to run this once before running the other cells\n",
    "\n",
    "\n",
    "#sklearn is scikit-learn\n",
    "import sklearn\n",
    "import csv\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all file paths\n",
    "#adapt path if needed\n",
    "\n",
    "# original training/dev data\n",
    "#Setting some variables that we will use multiple times\n",
    "trainfile = '../data/conll2003.train.conll'\n",
    "testfile = '../data/conll2003.dev.conll'\n",
    "\n",
    "# train/dev data after feature engineering\n",
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/dev.csv'\n",
    "feature_to_index = {\n",
    "    'token': 1,\n",
    "    'pos': 2,\n",
    "    'chunk_tag': 3,\n",
    "    'token_right': 5,\n",
    "    'token_left': 6,\n",
    "    'cap_type': 7\n",
    "}\n",
    "\n",
    "# word2vec model\n",
    "# this step takes a while\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '../data/GoogleNews-vectors-negative300.bin',\n",
    "    binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Traditional Features\n",
    "\n",
    "In this first part, we will explore the impact of various features on named entity recognition.\n",
    "We will use so-called traditional features, where the feature values (strings) are presented by one-hot encoding \n",
    "\n",
    "## Step 1: A Basic Classifier\n",
    "\n",
    "We will first walk through the process of creating and evaluating a simple classifier that only uses the token itself as a feature. In the next step, we will run evaluations on this basic system.\n",
    "\n",
    "This is generally a good way to start experimenting: first walk through the entire experimental process with a very basic, easy to create system to see if everything works, there are no problems with the data etc. You can then build up from there towards a more sophististicated system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for feature extraction and training a classifier\n",
    "\n",
    "\n",
    "## For documentation on how to create input representations of features in scikit-learn:\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "\n",
    "def extract_features_token_only_and_labels(conllfile):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter='\\t',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if len(row) == 4:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {'Token': row[0]}\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer).\n",
    "            labels.append(row[-1])\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "def create_vectorizer_and_classifier(features, labels):\n",
    "    '''\n",
    "    Function that takes feature-value pairs and gold labels as input and trains a logistic regression classifier\n",
    "    \n",
    "    :param features: feature-value pairs\n",
    "    :param labels: gold labels\n",
    "    :type features: a list of dictionaries\n",
    "    :type labels: a list of strings\n",
    "    \n",
    "    :return lr_classifier: a trained LogisticRegression classifier\n",
    "    :return vec: a DictVectorizer to which the feature values are fitted. \n",
    "    '''\n",
    "\n",
    "    vec = DictVectorizer()\n",
    "    #fit creates a mapping between observed feature values and dimensions in a one-hot vector, transform represents the current values as a vector\n",
    "    tokens_vectorized = vec.fit_transform(features)\n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(tokens_vectorized, labels)\n",
    "\n",
    "    return lr_classifier, vec\n",
    "\n",
    "#extract features and labels:\n",
    "feature_values, labels = extract_features_token_only_and_labels(trainfile)\n",
    "#create vectorizer and trained classifier:\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Evaluation\n",
    "\n",
    "We will now run a basic evaluation of the system on a test file. \n",
    "Two important properties of the test file:\n",
    "\n",
    "1. the test file and training file are independent sets (if they contain identical examples, this is coincidental)\n",
    "2. the test file is preprocessed in the exact same way as the training file \n",
    "\n",
    "The first function runs our classifier on the test data.\n",
    "\n",
    "The second function prints out a confusion matrix (comparing predictions and gold labels per class). \n",
    "You can find more information on confusion matrices here: https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "\n",
    "The third function prints out the macro precision, recall and f-score of the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1305      15    101      4      7       0      6      4    395\n",
      "B-MISC        41     603     14      8      0      12      2      1    241\n",
      "B-ORG         78      23    690      5     11       3     38     14    479\n",
      "B-PER         16       3      2    873      0       0      1    104    843\n",
      "I-LOC         13       2      1      0    150       3     13      6     69\n",
      "I-MISC         2      27      7      2      7     145      2      4    150\n",
      "I-ORG         36      11     47      4     38       5    263      5    342\n",
      "I-PER          6       2      5    102      0       0      1    292    899\n",
      "O              3      10      4      0      1      10     11      1  42719\n",
      "P: 0.8114098816314947 R: 0.5475891713668707 F1: 0.6374572803907079\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_predicted_and_gold_labels_token_only(testfile, vectorizer, classifier):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "    \n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    sparse_feature_reps, goldlabels = extract_features_token_only_and_labels(testfile)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(sparse_feature_reps)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "def print_confusion_matrix(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out a confusion matrix\n",
    "    \n",
    "    :param predictions: predicted labels\n",
    "    :param goldlabels: gold standard labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    #based on example from https://datatofish.com/confusion-matrix-python/ \n",
    "    data = {'Gold':    goldlabels, 'Predicted': predictions    }\n",
    "    df = pd.DataFrame(data, columns=['Gold','Predicted'])\n",
    "\n",
    "    confusion_matrix = pd.crosstab(df['Gold'], df['Predicted'], rownames=['Gold'], colnames=['Predicted'])\n",
    "    print (confusion_matrix)\n",
    "\n",
    "\n",
    "def print_precision_recall_fscore(predictions, goldlabels):\n",
    "    '''\n",
    "    Function that prints out precision, recall and f-score\n",
    "    \n",
    "    :param predictions: predicted output by classifier\n",
    "    :param goldlabels: original gold labels\n",
    "    :type predictions, goldlabels: list of strings\n",
    "    '''\n",
    "    \n",
    "    precision = metrics.precision_score(y_true=goldlabels,\n",
    "                        y_pred=predictions,\n",
    "                        average='macro')\n",
    "\n",
    "    recall = metrics.recall_score(y_true=goldlabels,\n",
    "                     y_pred=predictions,\n",
    "                     average='macro')\n",
    "\n",
    "\n",
    "    fscore = metrics.f1_score(y_true=goldlabels,\n",
    "                 y_pred=predictions,\n",
    "                 average='macro')\n",
    "\n",
    "    print('P:', precision, 'R:', recall, 'F1:', fscore)\n",
    "    \n",
    "#vectorizer and lr_classifier are the vectorizer and classifiers created in the previous cell.\n",
    "#it is important that the same vectorizer is used for both training and testing: they should use the same mapping from values to dimensions\n",
    "predictions, goldlabels = get_predicted_and_gold_labels_token_only(testfile, vectorizer, lr_classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: A More Elaborate System\n",
    "\n",
    "Now that we have run a basic experiment, we are going to investigate alternatives. In this exercise, we only focus on features. We will continue to use the same logistic regression classifier throughout the exercise.\n",
    "\n",
    "We want to investigate the impact of individual features. We will thus use a function that allows us to specify whether we include a specific feature or not. The features we have at our disposal are:\n",
    "\n",
    "* the token itself (as used above)\n",
    "* the preceding token\n",
    "* the capitalization indication (see above for values that this takes)\n",
    "* the pos-tag of the token\n",
    "* the chunklabel of the chunk the token is part of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1288      22     66     43      2       0      9     14    149\n",
      "B-MISC        24     692     35     22      0       2      6     14     79\n",
      "B-ORG         50      23    957     72      0       1     26     32     62\n",
      "B-PER         41       7     23   1325      0       1      8     25     63\n",
      "I-LOC          6       0      2      0    165       3     14     20     23\n",
      "I-MISC         9      20      3      3      5     212      6     25     51\n",
      "I-ORG         17       2     17      5     11       6    510     45     96\n",
      "I-PER          5       1      5     16      0       3      9    971     44\n",
      "O              4      14     25     38      0       5     21     16  37566\n",
      "P: 0.8846445875479344 R: 0.8056081274760225 F1: 0.8397141262252669\n"
     ]
    }
   ],
   "source": [
    "# the functions with multiple features and analysis\n",
    "\n",
    "#defines the column in which each feature is located (note: you can also define headers and use csv.DictReader)\n",
    "#feature_to_index = {'Token': 0, 'Prevtoken': 1, 'Cap': 2, 'Pos': 3, 'Chunklabel': 4}\n",
    "feature_to_index = {'token':1, 'pos':2, 'chunk_tag':3, 'token_left':5, 'token_right':6, 'cap_type':7}\n",
    "#['', 'token', 'pos', 'chunk_tag', 'target', 'token_left', 'token_right', 'cap_type']\n",
    "\n",
    "\n",
    "def extract_features_and_gold_labels(conllfile, selected_features):\n",
    "    '''Function that extracts features and gold label from preprocessed conll (here: tokens only).\n",
    "    \n",
    "    :param conllfile: path to the (preprocessed) conll file\n",
    "    :type conllfile: string\n",
    "    \n",
    "    \n",
    "    :return features: a list of dictionaries, with key-value pair providing the value for the feature `token' for individual instances\n",
    "    :return labels: a list of gold labels of individual instances\n",
    "    '''\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    conllinput = open(conllfile, 'r')\n",
    "    #delimiter indicates we are working with a tab separated value (default is comma)\n",
    "    #quotechar has as default value '\"', which is used to indicate the borders of a cell containing longer pieces of text\n",
    "    #in this file, we have only one token as text, but this token can be '\"', which then messes up the format. We set quotechar to a character that does not occur in our file\n",
    "    csvreader = csv.reader(conllinput, delimiter=',',quotechar='|')\n",
    "\n",
    "\n",
    "    for index,row in enumerate(csvreader):\n",
    "        #I preprocessed the file so that all rows with instances should contain 6 values, the others are empty lines indicating the beginning of a sentence\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if len(row) == 8:\n",
    "            #structuring feature value pairs as key-value pairs in a dictionary\n",
    "            #the first column in the conll file represents tokens\n",
    "            feature_value = {}\n",
    "            for feature_name in selected_features:\n",
    "                row_index = feature_to_index.get(feature_name)\n",
    "                feature_value[feature_name] = row[row_index]\n",
    "            features.append(feature_value)\n",
    "            #The last column provides the gold label (= the correct answer).\n",
    "            labels.append(row[4])\n",
    "    return features, labels\n",
    "\n",
    "def get_predicted_and_gold_labels(testfile, vectorizer, classifier, selected_features):\n",
    "    '''\n",
    "    Function that extracts features and runs classifier on a test file returning predicted and gold labels\n",
    "    \n",
    "    :param testfile: path to the (preprocessed) test file\n",
    "    :param vectorizer: vectorizer in which the mapping between feature values and dimensions is stored\n",
    "    :param classifier: the trained classifier\n",
    "    :type testfile: string\n",
    "    :type vectorizer: DictVectorizer\n",
    "    :type classifier: LogisticRegression()\n",
    "    \n",
    "    \n",
    "    \n",
    "    :return predictions: list of output labels provided by the classifier on the test file\n",
    "    :return goldlabels: list of gold labels as included in the test file\n",
    "    '''\n",
    "\n",
    "    #we use the same function as above (guarantees features have the same name and form)\n",
    "    features, goldlabels = extract_features_and_gold_labels(testfile, selected_features)\n",
    "    #we need to use the same fitting as before, so now we only transform the current features according to this mapping (using only transform)\n",
    "    test_features_vectorized = vectorizer.transform(features)\n",
    "    predictions = classifier.predict(test_features_vectorized)\n",
    "\n",
    "    return predictions, goldlabels\n",
    "\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "all_features = ['token', 'pos', 'chunk_tag', 'token_left', 'token_right','cap_type']\n",
    "\n",
    "sparse_feature_reps, labels = extract_features_and_gold_labels(train_path, all_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(sparse_feature_reps, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(test_path, vectorizer, lr_classifier, all_features)\n",
    "\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Ablation Analysis\n",
    "\n",
    "If all worked well, the system that made use of all features worked better than the system with just the tokens.\n",
    "We now want to know which of the features contributed to this improved: do we want to include all features?\n",
    "Or just some?\n",
    "\n",
    "We can investigate this using *feature ablation analysis*. This means that we systematically test what happens if we add or remove a specific feature. Ideally, we investigate all possible combinations.\n",
    "\n",
    "The cell below illustrates how you can use the code above to investigate a system with three features. You can modify the selected features to try out different combinations. You can either do this manually and rerun the cell or write a function that creates list of all combinations you want to tests and runs them one after the other.\n",
    "\n",
    "Include your results in the report of this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1246      10     93    118      8       0     12     45     61\n",
      "B-MISC        31     589     26     31      0      12     11     55    119\n",
      "B-ORG         73      21    716    114      4       2     60    113    120\n",
      "B-PER         14       1      7   1148      0       0      9    227     87\n",
      "I-LOC          6       2      1      1    134       3     32     39     15\n",
      "I-MISC         3      26      7      6      8     157      8     54     65\n",
      "I-ORG         33       6     52     13     23       5    354     95    128\n",
      "I-PER          1       2      4     41      0       0     11    957     38\n",
      "O              2       8     16     67      0       9     15    186  37386\n",
      "P: 0.7888709858928774 R: 0.6949827135561947 F1: 0.7245114308569229\n"
     ]
    }
   ],
   "source": [
    "#{'token':1, 'pos':2, 'chunk_tag':3, 'token_left':5, 'token_right':6, 'cap_type':7}\n",
    "# example of system with just one additional feature\n",
    "#define which from the available features will be used (names must match key names of dictionary feature_to_index)\n",
    "selected_features = ['token', 'pos', 'chunk_tag']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(train_path, selected_features)\n",
    "#we can use the same function as before for creating the classifier and vectorizer\n",
    "lr_classifier, vectorizer = create_vectorizer_and_classifier(feature_values, labels)\n",
    "#when applying our model to new data, we need to use the same features\n",
    "predictions, goldlabels = get_predicted_and_gold_labels(test_path, vectorizer, lr_classifier, selected_features)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'pos', 'chunk_tag')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1246      10     93    118      8       0     12     45     61\n",
      "B-MISC        31     589     26     31      0      12     11     55    119\n",
      "B-ORG         73      21    716    114      4       2     60    113    120\n",
      "B-PER         14       1      7   1148      0       0      9    227     87\n",
      "I-LOC          6       2      1      1    134       3     32     39     15\n",
      "I-MISC         3      26      7      6      8     157      8     54     65\n",
      "I-ORG         33       6     52     13     23       5    354     95    128\n",
      "I-PER          1       2      4     41      0       0     11    957     38\n",
      "O              2       8     16     67      0       9     15    186  37386\n",
      "P: 0.7888709858928774 R: 0.6949827135561947 F1: 0.7245114308569229\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'pos', 'token_left')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1231       6     61    110      4       1      6     25    149\n",
      "B-MISC        21     650     10     45      0      12      6      7    123\n",
      "B-ORG         66      19    801    114      1       3     68     48    103\n",
      "B-PER         17       2     20   1253      0       1      5     81    114\n",
      "I-LOC         28       1      6     10    131       5     17      9     26\n",
      "I-MISC         2      43      3     27      1     172      1      2     83\n",
      "I-ORG         31       6     78     44      8       5    377     24    136\n",
      "I-PER         26       1     15    225      0       1      0    676    110\n",
      "O             13       5     16    127      1       4     14     14  37495\n",
      "P: 0.8247908252203522 R: 0.6950905639767825 F1: 0.74521958218065\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'pos', 'token_right')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1326      12     69     71      3       0      3     34     75\n",
      "B-MISC        38     601     49     24      0       5      6     19    132\n",
      "B-ORG         78      21    861     85      0       1     18     42    117\n",
      "B-PER         48       2     31   1228      0       0      4     87     93\n",
      "I-LOC          9       2      1      0    157       2     19     30     13\n",
      "I-MISC         6       8     10      1      7     195     12     34     61\n",
      "I-ORG         26       5      6      6     29       7    441     62    127\n",
      "I-PER          4       0      5      9      0       0     12    988     36\n",
      "O              8       9     34     47      0       6     29    137  37419\n",
      "P: 0.8443571761301695 R: 0.7618243008856151 F1: 0.793045657952427\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'pos', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1273      22     90    103      6       0     14      4     81\n",
      "B-MISC        34     630     25     52      0       7     17      3    106\n",
      "B-ORG         76      22    799    108      9       3     67     17    122\n",
      "B-PER         16      16      6   1270      0       0      9     92     84\n",
      "I-LOC         17       2      1     26    128       3     32      5     19\n",
      "I-MISC         2      33     10     37      9     144      8      6     85\n",
      "I-ORG         40       3     60     84     25       5    347      8    137\n",
      "I-PER         10       4      5    546      0       0     10    420     59\n",
      "O              6      29     16     49      1       8     23      6  37551\n",
      "P: 0.777156497451169 R: 0.6542920979487996 F1: 0.6949804569101775\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'chunk_tag', 'token_left')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1147       4     46      9      4       0      2      6    375\n",
      "B-MISC        12     626      3      6      0      11      1      1    214\n",
      "B-ORG         47      19    734     12      1       1     48     12    349\n",
      "B-PER         13       1     17    799      0       0      0     16    647\n",
      "I-LOC          7       0      1      0    137       5     14      5     64\n",
      "I-MISC         1      37      1      1      0     169      0      1    124\n",
      "I-ORG         22       6     37      1      8       5    349     17    264\n",
      "I-PER          4       0      3     29      0       0      0    582    436\n",
      "O              1       2     12      4      0       7     17     47  37599\n",
      "P: 0.8849921200287638 R: 0.6341786145749984 F1: 0.7319999052014984\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'chunk_tag', 'token_right')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1206      18     50     16      2       0      9      0    292\n",
      "B-MISC        34     576     15      6      0       3      1      0    239\n",
      "B-ORG         43      22    734     10      0       0      6      1    407\n",
      "B-PER         13       1     14    919      0       0      2      5    539\n",
      "I-LOC          6       2      0      0    158       2     14      8     43\n",
      "I-MISC         2       7      0      0      7     193      3      3    119\n",
      "I-ORG         15       5      3      1     28       7    385      7    258\n",
      "I-PER          0       0      0      5      0       0      2    703    344\n",
      "O              3       7     13      3      1       2     13      2  37645\n",
      "P: 0.9099408769372482 R: 0.6773992746113658 F1: 0.7707006274706293\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'chunk_tag', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1241      18     94     86      8       0     11     45     90\n",
      "B-MISC        38     610     20     42      0      11      3     62     88\n",
      "B-ORG         75      24    764     96      4       2     64     88    106\n",
      "B-PER         15       2      4   1202      0       0      6    215     49\n",
      "I-LOC          6       2      1      0    134       3     32     38     17\n",
      "I-MISC         2      27      7      4      8     147      6     49     84\n",
      "I-ORG         31      11     52     11     23       5    347     99    130\n",
      "I-PER          1       2      4     43      0       0      3    949     52\n",
      "O              4       8     16    100      0      11     18     43  37489\n",
      "P: 0.7961848095840558 R: 0.7007199187515929 F1: 0.7325121385037207\n",
      "\n",
      "\n",
      "-----------('token', 'token_left', 'token_right')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1147       8     25     17      1       0      1      1    393\n",
      "B-MISC        15     619      8      5      0       4      1      1    221\n",
      "B-ORG         18      18    776     19      0       0     12      3    377\n",
      "B-PER         12       1      6    975      0       0      0      8    491\n",
      "I-LOC          7       0      0      0    158       2      6      5     55\n",
      "I-MISC         2      25      0      0      1     191      3      1    111\n",
      "I-ORG         19       4      5      1     10       5    442     12    211\n",
      "I-PER          2       0      0     10      0       0      0    738    304\n",
      "O              0       3     14      8      0       2      7     19  37636\n",
      "P: 0.9342684725824044 R: 0.698664511907515 F1: 0.7948708360487059\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'token_left', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1231      11     62     68      4       0      3     29    185\n",
      "B-MISC        23     674      7     55      0      12      3     15     85\n",
      "B-ORG         61      19    835    100      1       1     68     48     90\n",
      "B-PER         32       2     21   1287      0       1      4     96     50\n",
      "I-LOC         28       1      6     10    132       5     15     10     26\n",
      "I-MISC         5      45      1     26      1     174      0      3     79\n",
      "I-ORG         38       6     66     48      8       5    379     25    134\n",
      "I-PER         58       0     10    230      0       1      0    677     78\n",
      "O             16       7     14     92      2       8     11     25  37514\n",
      "P: 0.824723962208154 R: 0.7053780108224728 F1: 0.7514205407824479\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token', 'token_right', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1327      21     76     32      4       0      3     36     94\n",
      "B-MISC        42     622     52     21      0       5      4     26    102\n",
      "B-ORG         74      25    905     46      0       0     20     45    108\n",
      "B-PER         50       3     34   1151      0       0      3     89    163\n",
      "I-LOC          6       2      1      0    160       2     19     27     16\n",
      "I-MISC         7      10     11      4      7     197      9     34     55\n",
      "I-ORG         25       5      8      5     28       8    459     62    109\n",
      "I-PER          5       0      5      8      0       0      6    979     51\n",
      "O              9       8     28     25      0       9     24     25  37561\n",
      "P: 0.8564185039912492 R: 0.7672174685056465 F1: 0.8035107779624737\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('pos', 'chunk_tag', 'token_left')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        853      15     46    252     11       2      7     24    383\n",
      "B-MISC        25     284     16     88      7       9     21     23    401\n",
      "B-ORG        145      19    467    165      2       6    120     43    256\n",
      "B-PER         98       3     30   1154      1       2     10     22    173\n",
      "I-LOC         31       2      6     23     36       2     19     48     66\n",
      "I-MISC        11      41      2     31      5      59      6     17    162\n",
      "I-ORG         28       9     50     50     12       4    250    114    192\n",
      "I-PER          9       2      5    111     19       5     12    733    158\n",
      "O            105      71     56    240      8       9     24     25  37151\n",
      "P: 0.6299312263402735 R: 0.48668137974918463 F1: 0.5255604858464089\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('pos', 'chunk_tag', 'token_right')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        973       7    162    148      0       0      2     39    262\n",
      "B-MISC        56      40    116     55      0       4     13     38    552\n",
      "B-ORG        192      23    603    109      0       0      3     54    239\n",
      "B-PER        280       6     78    858      0       3      4     92    172\n",
      "I-LOC          4       0      2      0    137       2     26     46     16\n",
      "I-MISC         6       2      9      7      9     117     33     56     95\n",
      "I-ORG         39       2      2     13     39      17    327    132    138\n",
      "I-PER          5       0      4      2      0       1     12    987     43\n",
      "O            201       9    101     79      2      15     29    183  37070\n",
      "P: 0.6669516332210849 R: 0.5604221679369109 F1: 0.5788235086069191\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('pos', 'chunk_tag', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  I-ORG  I-PER      O\n",
      "Gold                                                \n",
      "B-LOC       1193      28      4      2    197    169\n",
      "B-MISC       145     388     15     24    164    138\n",
      "B-ORG        626      29     11     18    298    241\n",
      "B-PER        763      20      5      4    618     83\n",
      "I-LOC          5       3      0      2    170     53\n",
      "I-MISC        14      15      0      8    173    124\n",
      "I-ORG         51       3      2     32    456    165\n",
      "I-PER         16       3      4     16    941     74\n",
      "O            654      82      6      3    173  36771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.31317008805878843 R: 0.3461553080887 F1: 0.2801726295012383\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('pos', 'token_left', 'token_right')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1051      14     89    173      1       0      7     33    225\n",
      "B-MISC        39     306     44     38      0       3      8     10    426\n",
      "B-ORG        132      31    725    104      0       2     13     19    197\n",
      "B-PER         89       4     53   1127      0       4      4     20    192\n",
      "I-LOC          9       1      5      0    140       4     18     14     42\n",
      "I-MISC         9      23      5      2      4     154     18      6    113\n",
      "I-ORG         38       2     13      7     14      12    412     47    164\n",
      "I-PER         13       1      3     14      1       3      4    898    117\n",
      "O            146      57     74     81      2      10     24     18  37277\n",
      "P: 0.7904740895307667 R: 0.649070404352056 F1: 0.7027946043151473\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('pos', 'token_left', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        861      36    159    177      2       2      6     29    321\n",
      "B-MISC        34     556     32     91      3       8     20      8    122\n",
      "B-ORG        151      28    589    154      0       4     96     54    147\n",
      "B-PER        111      21     75   1133      0       3      3     33    114\n",
      "I-LOC         99       3     13     35      4       1      3     12     63\n",
      "I-MISC        24      49     12     51      0      69      3      9    117\n",
      "I-ORG         90      17    142    121      0       1    132     45    161\n",
      "I-PER        122       6     11    222      0       4      2    548    139\n",
      "O            109      67     98    175      0       4     13     39  37184\n",
      "P: 0.6228742877033762 R: 0.48150883344392553 F1: 0.5032192468107788\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('pos', 'token_right', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        990      28    198    120      1       0      1    105    150\n",
      "B-MISC        64     439    138     50      0       6      9     42    126\n",
      "B-ORG        198      45    668    101      0       1      5     61    144\n",
      "B-PER        253      21     89    908      0       3      3    127     89\n",
      "I-LOC          4       3      2      0    136       2     23     41     22\n",
      "I-MISC         9       6      9      8      9     130     32     48     83\n",
      "I-ORG         42       4      4     13     39      17    332    126    132\n",
      "I-PER          5       4      5      1      0       1     11    969     58\n",
      "O            190      98    141     75      1      14     35     67  37068\n",
      "P: 0.6988953946721076 R: 0.6246869095309038 F1: 0.6457873795717668\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('chunk_tag', 'token_left', 'token_right')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        623       2     31     22      0       0      3      3    909\n",
      "B-MISC        10     176      3      2      0       3      0      0    680\n",
      "B-ORG         52       9    387     11      0       0      5      2    757\n",
      "B-PER         22       0      7    601      0       1      1      0    861\n",
      "I-LOC          1       0      0      0    127       5     14      4     82\n",
      "I-MISC         0      13      0      0     15     110      7      7    182\n",
      "I-ORG         15       0      1      3     11       5    302     16    356\n",
      "I-PER          3       0      0      0      0       0      1    722    328\n",
      "O            145      27     67     31      7      19     35     45  37313\n",
      "P: 0.8172135332646592 R: 0.47631444978092496 F1: 0.5801935822953305\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('chunk_tag', 'token_left', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        827      45     33     72      5       2     14     49    546\n",
      "B-MISC        34     400     13    107      5       8     12     39    256\n",
      "B-ORG        138      21    504     83      2       6    114     46    309\n",
      "B-PER         96       4     31    873      0       2      6     32    449\n",
      "I-LOC         39      11      4     19     25       2     19     71     43\n",
      "I-MISC         8      52      3     31      5      80      5     41    109\n",
      "I-ORG         33      22     43     59     11       5    234    144    158\n",
      "I-PER         18       0      5    110     10       5     21    783    102\n",
      "O            188      66     73    120      2      10     18     36  37176\n",
      "P: 0.6340176346871568 R: 0.486641617042126 F1: 0.5308836239080277\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('chunk_tag', 'token_right', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        773     115     91    112      0       0      1     32    469\n",
      "B-MISC        82     248     79     84      0       5      1     88    287\n",
      "B-ORG        111     164    548     83      0       0      2     57    258\n",
      "B-PER        116      13     77    812      0       3      4     91    377\n",
      "I-LOC          4       0      4      0    136       2     23     42     22\n",
      "I-MISC         7       8      3      9     18     114     32     54     89\n",
      "I-ORG         42       1      3     14     37      18    326    134    134\n",
      "I-PER          5       0      5      2      0       1      4    971     66\n",
      "O            128      36    158     84      1      15     28     44  37195\n",
      "P: 0.6753943650624467 R: 0.5615451895581622 F1: 0.5990955307377868\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------('token_left', 'token_right', 'cap_type')---------------\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        986      58     71     84      2       1      9     26    356\n",
      "B-MISC        73     468     39     77      0       5      3     17    192\n",
      "B-ORG        114      67    720     90      0       2     16     20    194\n",
      "B-PER         90      13     49   1078      0       4      6     23    230\n",
      "I-LOC         13       2      2      3    136       8     22     20     27\n",
      "I-MISC         9      35      2     12      5     158     19     14     80\n",
      "I-ORG         43       5     12     21     15      14    422     55    122\n",
      "I-PER         18       0      3     37      1       3      4    908     80\n",
      "O            124      49     80    104      1      14     28     43  37246\n",
      "P: 0.7745616083917495 R: 0.6629838738587819 F1: 0.7087758786365671\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "#code taken from Google\n",
    "feature_combinations = list(itertools.combinations(feature_to_index.keys(), 3))\n",
    "for feature_c in feature_combinations:\n",
    "    selected_features = list(feature_c)\n",
    "    feature_values, labels = extract_features_and_gold_labels(train_path, selected_features)\n",
    "    lr_classifier, vectorizer = create_vectorizer_and_classifier(\n",
    "        feature_values, labels)\n",
    "    predictions, goldlabels = get_predicted_and_gold_labels(\n",
    "        test_path, vectorizer, lr_classifier, selected_features)\n",
    "    print(f'-----------{feature_c}---------------')\n",
    "    print_confusion_matrix(predictions, goldlabels)\n",
    "    print_precision_recall_fscore(predictions, goldlabels)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: One-hot versus Embeddings\n",
    "\n",
    "In this second part of the exercise, we will compare results using one-hot encodings to pretrained word embeddings.\n",
    "\n",
    "## One-hot representation\n",
    "\n",
    "In one-hot representation, each feature value is represented by an n-dimensional vector, where n corresponds to the number of possible values the feature can take. In our system, the Token feature can take the value of each token that occurs at least once in the corpus. This feature thus uses a vector with the size of the vocabulary in the corpus. Each possible value is associated with a specific dimension. If this value is represented, that dimension will receive the value 1 and all other dimensions will have the value 0.\n",
    "\n",
    "The system receive a concatenation of all feature representations as input.\n",
    "\n",
    "\n",
    "## What does one-hot look like?\n",
    "\n",
    "We will start with an illustration of a one-hot representation. We will use the capitalization feature for this: it has 6 possible values and is therefore represented by a 6-dimensional vector. If you would like a more precise look, you may consider creating a toy example of a few lines, in which the capitalization feature has different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# create classifier with caps feature only and print vectorizer, then with token only (but you see less)\n",
    "\n",
    "selected_features = ['cap_type']\n",
    "\n",
    "feature_values, labels = extract_features_and_gold_labels(train_path, selected_features)\n",
    "\n",
    "#creating a vectorizing\n",
    "vectorizer = DictVectorizer()\n",
    "#fitting the values to dimensions (creating a mapping) and transforming the current observations according to this mapping\n",
    "capitalization_vectorized = vectorizer.fit_transform(feature_values)\n",
    "print(capitalization_vectorized.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using word embeddings\n",
    "\n",
    "We are now going to use word embeddings to represent tokens. We load a pretrained distributional semantic model.\n",
    "You can use the same model as in Exercise 2.1. We tested the exercise with the same model (GoogleNews negative sampling 300 dimensions) as Exercise 2.1 as well.\n",
    "\n",
    "Note: loading the model may take a while. You probably want to run that only once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier....\n",
      "Running evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1288      22     66     43      2       0      9     14    149\n",
      "B-MISC        24     692     35     22      0       2      6     14     79\n",
      "B-ORG         50      23    957     72      0       1     26     32     62\n",
      "B-PER         41       7     23   1325      0       1      8     25     63\n",
      "I-LOC          6       0      2      0    165       3     14     20     23\n",
      "I-MISC         9      20      3      3      5     212      6     25     51\n",
      "I-ORG         17       2     17      5     11       6    510     45     96\n",
      "I-PER          5       1      5     16      0       3      9    971     44\n",
      "O              4      14     25     38      0       5     21     16  37566\n",
      "P: 0.7398747647969717 R: 0.6551042215507106 F1: 0.6907918605672456\n"
     ]
    }
   ],
   "source": [
    "def extract_embeddings_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter=',',quotechar='|')\n",
    "    for index, row in enumerate(csvreader):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if len(row) == 8:\n",
    "            if row[1] in word_embedding_model:\n",
    "                vector = word_embedding_model[row[1]]\n",
    "            else:\n",
    "                vector = [0]*300\n",
    "            features.append(vector)\n",
    "            labels.append(row[4])\n",
    "    return features, labels\n",
    "\n",
    "def create_classifier(features, labels):\n",
    "    '''\n",
    "    Function that creates classifier from features represented as vectors and gold labels\n",
    "    \n",
    "    :param features: list of vector representations of tokens\n",
    "    :param labels: list of gold labels\n",
    "    :type features: list of vectors\n",
    "    :type labels: list of strings\n",
    "    \n",
    "    :returns trained logistic regression classifier\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    lr_classifier = LogisticRegression(solver='saga')\n",
    "    lr_classifier.fit(features, labels)\n",
    "    \n",
    "    return lr_classifier\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features and gold labels from test data and runs a classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    dense_feature_representations, labels = extract_embeddings_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(dense_feature_representations)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "# I printing announcements of where the code is at (since some of these steps take a while)\n",
    "\n",
    "print('Extracting dense features...')\n",
    "dense_feature_representations, labels = extract_embeddings_as_features_and_gold(train_path,word_embedding_model)\n",
    "print('Training classifier....')\n",
    "classifier = create_classifier(dense_feature_representations, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings(test_path, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including the preceding token\n",
    "\n",
    "We can include the preceding token as a feature in a similar way. We simply concatenate the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dense features...\n",
      "Training classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC        986      58     71     84      2       1      9     26    356\n",
      "B-MISC        73     468     39     77      0       5      3     17    192\n",
      "B-ORG        114      67    720     90      0       2     16     20    194\n",
      "B-PER         90      13     49   1078      0       4      6     23    230\n",
      "I-LOC         13       2      2      3    136       8     22     20     27\n",
      "I-MISC         9      35      2     12      5     158     19     14     80\n",
      "I-ORG         43       5     12     21     15      14    422     55    122\n",
      "I-PER         18       0      3     37      1       3      4    908     80\n",
      "O            124      49     80    104      1      14     28     43  37246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 0.7646172743895793 R: 0.7090309354680027 F1: 0.7345602505082284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_embeddings_of_current_and_preceding_as_features_and_gold(conllfile,word_embedding_model):\n",
    "    '''\n",
    "    Function that extracts features and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter=',',quotechar='|')\n",
    "    for row in csvreader:\n",
    "        if len(row) == 8:\n",
    "            if row[1] in word_embedding_model:\n",
    "                vector1 = word_embedding_model[row[1]]\n",
    "            else:\n",
    "                vector1 = [0]*300\n",
    "            if row[6] in word_embedding_model:\n",
    "                vector2 = word_embedding_model[row[6]]\n",
    "            else:\n",
    "                vector2 = [0]*300\n",
    "            features.append(np.concatenate((vector1,vector2)))\n",
    "            labels.append(row[4])\n",
    "    return features, labels\n",
    "    \n",
    "    \n",
    "def label_data_using_word_embeddings_current_and_preceding(testfile, word_embedding_model, classifier):\n",
    "    '''\n",
    "    Function that extracts word embeddings as features (of current and preceding token) and gold labels from test data and runs a trained classifier\n",
    "    \n",
    "    :param testfile: path to test file\n",
    "    :param word_embedding_model: distributional semantic model\n",
    "    :param classifier: trained classifier\n",
    "    :type testfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    :type classifier: LogisticRegression\n",
    "    \n",
    "    :return predictions: list of predicted labels\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    \n",
    "    features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(testfile,word_embedding_model)\n",
    "    predictions = classifier.predict(features)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "\n",
    "print('Extracting dense features...')\n",
    "features, labels = extract_embeddings_of_current_and_preceding_as_features_and_gold(train_path,word_embedding_model)\n",
    "print('Training classifier...')\n",
    "#we can use the same function as for just the tokens itself\n",
    "classifier = create_classifier(features, labels)\n",
    "print('Running evaluation...')\n",
    "predicted, gold = label_data_using_word_embeddings_current_and_preceding(test_path, word_embedding_model, classifier)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predicted, gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A mixed system\n",
    "\n",
    "The code below combines traditional features with word embeddings. Note that we only include features with a limited range of possible values. Combining one-hot token representations (using highly sparse dimensions) with dense representations is generally not a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features...\n",
      "Training classifier....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lisa2021/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the evaluation...\n",
      "Predicted  B-LOC  B-MISC  B-ORG  B-PER  I-LOC  I-MISC  I-ORG  I-PER      O\n",
      "Gold                                                                      \n",
      "B-LOC       1395      28     91     24      5       0     17      2     31\n",
      "B-MISC        46     644     69     20      1       7      6      0     81\n",
      "B-ORG         88      41    974     49      4       3     13      0     51\n",
      "B-PER         35       8     28   1367      5       0      4     13     33\n",
      "I-LOC          6       0      3      0    174       6     25      3     16\n",
      "I-MISC         3      11      8      2     10     208     22      3     67\n",
      "I-ORG         20       5     14      6     29      23    466      9    137\n",
      "I-PER          1       1      5     42      3       0     11    964     27\n",
      "O              9      25     59     15      2      22     46      2  37509\n",
      "P: 0.8477110179619565 R: 0.8067990229479484 F1: 0.8253967328075144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_word_embedding(token, word_embedding_model):\n",
    "    '''\n",
    "    Function that returns the word embedding for a given token out of a distributional semantic model and a 300-dimension vector of 0s otherwise\n",
    "    \n",
    "    :param token: the token\n",
    "    :param word_embedding_model: the distributional semantic model\n",
    "    :type token: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :returns a vector representation of the token\n",
    "    '''\n",
    "    if token in word_embedding_model:\n",
    "        vector = word_embedding_model[token]\n",
    "    else:\n",
    "        vector = [0]*300\n",
    "    return vector\n",
    "\n",
    "\n",
    "def extract_feature_values(row, selected_features):\n",
    "    '''\n",
    "    Function that extracts feature value pairs from row\n",
    "    \n",
    "    :param row: row from conll file\n",
    "    :param selected_features: list of selected features\n",
    "    :type row: string\n",
    "    :type selected_features: list of strings\n",
    "    \n",
    "    :returns: dictionary of feature value pairs\n",
    "    '''\n",
    "    feature_values = {}\n",
    "    for feature_name in selected_features:\n",
    "        r_index = feature_to_index.get(feature_name)\n",
    "        feature_values[feature_name] = row[r_index]\n",
    "        \n",
    "    return feature_values\n",
    "    \n",
    "    \n",
    "def create_vectorizer_traditional_features(feature_values):\n",
    "    '''\n",
    "    Function that creates vectorizer for set of feature values\n",
    "    \n",
    "    :param feature_values: list of dictionaries containing feature-value pairs\n",
    "    :type feature_values: list of dictionairies (key and values are strings)\n",
    "    \n",
    "    :returns: vectorizer with feature values fitted\n",
    "    '''\n",
    "    vectorizer = DictVectorizer()\n",
    "    vectorizer.fit(feature_values)\n",
    "    \n",
    "    return vectorizer\n",
    "        \n",
    "    \n",
    "def combine_sparse_and_dense_features(dense_vectors, sparse_features):\n",
    "    '''\n",
    "    Function that takes sparse and dense feature representations and appends their vector representation\n",
    "    \n",
    "    :param dense_vectors: list of dense vector representations\n",
    "    :param sparse_features: list of sparse vector representations\n",
    "    :type dense_vector: list of arrays\n",
    "    :type sparse_features: list of lists\n",
    "    \n",
    "    :returns: list of arrays in which sparse and dense vectors are concatenated\n",
    "    '''\n",
    "    \n",
    "    combined_vectors = []\n",
    "    sparse_vectors = np.array(sparse_features.toarray())\n",
    "    \n",
    "    for index, vector in enumerate(sparse_vectors):\n",
    "        combined_vector = np.concatenate((vector,dense_vectors[index]))\n",
    "        combined_vectors.append(combined_vector)\n",
    "    return combined_vectors\n",
    "    \n",
    "\n",
    "def extract_traditional_features_and_embeddings_plus_gold_labels(conllfile, word_embedding_model, vectorizer=None):\n",
    "    '''\n",
    "    Function that extracts traditional features as well as embeddings and gold labels using word embeddings for current and preceding token\n",
    "    \n",
    "    :param conllfile: path to conll file\n",
    "    :param word_embedding_model: a pretrained word embedding model\n",
    "    :type conllfile: string\n",
    "    :type word_embedding_model: gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "    \n",
    "    :return features: list of vector representation of tokens\n",
    "    :return labels: list of gold labels\n",
    "    '''\n",
    "    labels = []\n",
    "    dense_vectors = []\n",
    "    traditional_features = []\n",
    "    \n",
    "    conllinput = open(conllfile, 'r')\n",
    "    csvreader = csv.reader(conllinput, delimiter=',',quotechar='|')\n",
    "    for index, row in enumerate(csvreader):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        if len(row) == 8:\n",
    "            token_vector = extract_word_embedding(row[1], word_embedding_model)\n",
    "            pt_vector = extract_word_embedding(row[6], word_embedding_model)\n",
    "            dense_vectors.append(np.concatenate((token_vector,pt_vector)))\n",
    "            #mixing very sparse representations (for one-hot tokens) and dense representations is a bad idea\n",
    "            #we thus only use other features with limited values\n",
    "            other_features = extract_feature_values(row, ['cap_type'])\n",
    "            traditional_features.append(other_features)\n",
    "            #adding gold label to labels\n",
    "            labels.append(row[4])\n",
    "            \n",
    "    #create vector representation of traditional features\n",
    "    if vectorizer is None:\n",
    "        #creates vectorizer that provides mapping (only if not created earlier)\n",
    "        vectorizer = create_vectorizer_traditional_features(traditional_features)\n",
    "    sparse_features = vectorizer.transform(traditional_features)\n",
    "    combined_vectors = combine_sparse_and_dense_features(dense_vectors, sparse_features)\n",
    "    \n",
    "    return combined_vectors, vectorizer, labels\n",
    "\n",
    "def label_data_with_combined_features(testfile, classifier, vectorizer, word_embedding_model):\n",
    "    '''\n",
    "    Function that labels data with model using both sparse and dense features\n",
    "    '''\n",
    "    feature_vectors, vectorizer, goldlabels = extract_traditional_features_and_embeddings_plus_gold_labels(testfile, word_embedding_model, vectorizer)\n",
    "    predictions = classifier.predict(feature_vectors)\n",
    "    \n",
    "    return predictions, goldlabels\n",
    "\n",
    "\n",
    "print('Extracting Features...')\n",
    "feature_vectors, vectorizer, gold_labels = extract_traditional_features_and_embeddings_plus_gold_labels(train_path, word_embedding_model)\n",
    "print('Training classifier....')\n",
    "lr_classifier = create_classifier(feature_vectors, gold_labels)\n",
    "print('Running the evaluation...')\n",
    "predictions, goldlabels = label_data_with_combined_features(test_path, lr_classifier, vectorizer, word_embedding_model)\n",
    "print_confusion_matrix(predictions, goldlabels)\n",
    "print_precision_recall_fscore(predictions, goldlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b404d2aafdf561b3056e6883553e066f5a70fc5deb88f06afbbb68722dddb5be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
