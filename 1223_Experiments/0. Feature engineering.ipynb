{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages and data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from util.feature_extract import extract_left_token, extract_right_token, cap_type\n",
    "\n",
    "#import from our util\n",
    "\n",
    "#conll filepath\n",
    "train_path = '../data/conll2003.train.conll'\n",
    "dev_path = '../data/conll2003.dev.conll'\n",
    "\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(train_path, delimiter='\\t', quotechar='|', header=None)\n",
    "train = train.rename({0: 'token', 1: 'pos', 2: 'chunk_tag', 3: 'target'}, axis=1)\n",
    "# [row, column]\n",
    "X_train = train[['token', 'pos', 'chunk_tag']]\n",
    "Y_train = train['target']\n",
    "#print(Y_train)\n",
    "\n",
    "# Load the test data\n",
    "dev = pd.read_csv(dev_path, delimiter='\\t', quotechar='|', header=None)\n",
    "dev = dev.rename({0: 'token', 1: 'pos', 2: 'chunk_tag', 3: 'target'}, axis=1)\n",
    "X_dev = dev[['token', 'pos', 'chunk_tag']]\n",
    "Y_dev = dev['target']\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit_transform(X_train)\n",
    "X_train_ohe = enc.transform(X_train)\n",
    "X_dev_ohe = enc.transform(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # word2vec\n",
    "# from util import word2vec\n",
    "# #Word embedding\n",
    "# X_train['word_embedding'] = X_train['token'].apply(word2vec)\n",
    "# X_test['word_embedding'] = X_test['token'].apply(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data (adding new features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add token in the left to features\n",
    "train['token_right'] = extract_right_token(train['token'])\n",
    "train['token_left'] = extract_left_token(train['token'])\n",
    "train['cap_type'] = cap_type(train['token'])\n",
    "\n",
    "\n",
    "\n",
    "dev['token_right'] = extract_right_token(dev['token'])\n",
    "dev['token_left'] = extract_left_token(dev['token'])\n",
    "dev['cap_type'] = cap_type(dev['token'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../data/train.csv', encoding='utf-8')\n",
    "dev.to_csv('../data/dev.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b404d2aafdf561b3056e6883553e066f5a70fc5deb88f06afbbb68722dddb5be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
